executionFrameworkSpec:
  name: 'hadoop'
  segmentGenerationJobRunnerClassName: 'org.apache.pinot.plugin.ingestion.batch.hadoop.HadoopSegmentGenerationJobRunner'
  segmentTarPushJobRunnerClassName: 'org.apache.pinot.plugin.ingestion.batch.hadoop.HadoopSegmentTarPushJobRunner'
  segmentUriPushJobRunnerClassName: 'org.apache.pinot.plugin.ingestion.batch.hadoop.HadoopSegmentUriPushJobRunner'
  extraConfigs:
    stagingDir: 'hdfs://nn-hadoop.apache-hadoop.apache.svc.cluster.local:8020/pp_table_segment'
jobType: SegmentCreationAndTarPush
inputDirURI: 'hdfs://nn-hadoop.apache-hadoop.apache.svc.cluster.local:8020/pp_table'
includeFileNamePattern: 'glob:**/*.csv'
outputDirURI: 'hdfs://nn-hadoop.apache-hadoop.apache.svc.cluster.local:8020/pp_table_output'
overwriteOutput: true
pinotFSSpecs:
  - scheme: hdfs
    className: org.apache.pinot.plugin.filesystem.HadoopPinotFS

recordReaderSpec:
  dataFormat: 'csv'
  className: 'org.apache.pinot.plugin.inputformat.csv.CSVRecordReader'
tableSpec:
  tableName: 'pp_table'
  schemaURI: 'http://10.244.0.166:9000/tables/pp_table/schema'
  tableConfigURI: 'http://10.244.0.166:9000/tables/pp_table'
segmentNameGeneratorSpec:
  type: normalizedDate
pinotClusterSpecs:
  - controllerURI: 'http://10.244.0.166:9000'
pushJobSpec:
  pushParallelism: 2
  pushAttempts: 2
  pushRetryIntervalMillis: 1000